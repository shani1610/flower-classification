{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning and Computer Vision\n",
    "## Regularization, Data augmentation and transfer learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this exercise is to code a CNN that provides the best results you can on a classification\n",
    "task. The idea is to test the different tools studied during the lecture and to measure their impact on\n",
    "the provided dataset.\n",
    "This dataset is constituted by 10 categories of flowers.\n",
    "It contains 800 color images: 600 (60 per category) are in the training set and 200 (20 per category)\n",
    "in the validation set. The train/validation split is provided.\n",
    "The images have different sizes, so it is required to resize them to 128x128.\n",
    "For this exercise, you have to:\n",
    "- Work only with images whose sizes are 128x128.\n",
    "- Train your network only on the training set (not on the validation set).\n",
    "- Cite the references of any code found online and be able to explain it.\n",
    "You have to improve the performance of the provided network step by step.\n",
    "\n",
    "You have to submit a report for this exercise (groups from 1 to 3 students).\n",
    "The deadline is March 28th\n",
    " for the report.\n",
    "Each time you have an idea to improve the results, you have to create a new section in your report\n",
    "in which you:\n",
    "- Give some details about your idea.\n",
    "- Explain why do you think it could be good to test this idea.\n",
    "- Provide the code for this idea.\n",
    "- Provide the results.\n",
    "At least, the next steps have to be explored (maybe not in this order):\n",
    "- Your own architecture trained from scratch.\n",
    "- Some regularization tools.\n",
    "- Data augmentation.\n",
    "- Fine tuning a pre-trained CNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#from tensorflow.keras import layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_ds = keras.utils.image_dataset_from_directory(\n",
    "    directory='data/Flowers/Train',\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=32,\n",
    "    image_size=(128, 128))\n",
    "validation_ds = keras.utils.image_dataset_from_directory(\n",
    "    directory='data/Flowers/Test',\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=32,\n",
    "    image_size=(128, 128))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we use the MNIST dataset which consists of 28x28 grayscale images of handwritten digits. We first load the dataset and normalize the pixel values between 0 and 1. We then add a channels dimension to the images to make them compatible with the CNN architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values between 0 and 1\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add a channels dimension\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model architecture consists of two convolutional layers with max pooling, followed by a flattening layer, dropout layer, and a dense output layer with softmax activation.\n",
    "\n",
    "We compile the model using the sparse categorical crossentropy loss function, the Adam optimizer, and the accuracy metric. We then train the model on the training set, using a batch size of 128 and 5 epochs.\n",
    "\n",
    "Finally, we evaluate the model on the test set and print the test loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=5, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
